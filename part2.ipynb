{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBUUapXTBBbv/EqWL8ycCY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TE-Raven/python-project/blob/main/part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import moduls"
      ],
      "metadata": {
        "id": "vHjyjYhRqpHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXdbsy_3p2O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8418d590-dde8-4de9-b2bd-de551e0b19d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim import utils\n",
        "import gensim.parsing.preprocessing as gsp\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import sklearn\n",
        "import operator\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataprocessing"
      ],
      "metadata": {
        "id": "qJxnVtbRqmkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root=r\"/content/drive/MyDrive/news_classifacation/bbc\"\n",
        "filename=[]\n",
        "dirs=os.listdir(root)\n",
        "mark=0\n",
        "data= pd.DataFrame(columns=('text','mark'))\n",
        "#print(dirs)\n",
        "for dir in dirs:\n",
        "    dir_path = root +'/' +dir\n",
        "    names = os.listdir(dir_path)\n",
        "    if dir == 'business':\n",
        "        mark =0\n",
        "    if dir == 'entertainment':\n",
        "        mark =1\n",
        "    if dir == 'politics':\n",
        "        mark =2\n",
        "    if dir == 'sport':\n",
        "        mark =3\n",
        "    if dir == 'tech':\n",
        "        mark =4\n",
        "    for n in names:\n",
        "     filename.append(dir_path + '/'+ n )\n",
        "    for m in filename:\n",
        "        with open(m, encoding='gb18030', errors='ignore') as content:\n",
        "            # text=content.rstrip('\\n')\n",
        "            a = content.read()\n",
        "            b = a.replace('\\n', '').replace('\\r', '')\n",
        "            data1 = pd.DataFrame({'text': b, \"mark\": mark}, index=[0])\n",
        "            data = data.append(data1, ignore_index=True)\n",
        "    filename=[]\n",
        "#data.to_csv(\"dataset.csv\") this command can generate a dataset file for check or other using\n"
      ],
      "metadata": {
        "id": "dV2m86M5p5vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=data[['text','mark']]\n",
        "lable=data['mark']"
      ],
      "metadata": {
        "id": "u3WuIwgus35h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set filters"
      ],
      "metadata": {
        "id": "zc7dsoJtCtuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filters = [\n",
        "               gsp.strip_punctuation,          \n",
        "               gsp.strip_multiple_whitespaces,  \n",
        "               gsp.strip_numeric,              \n",
        "               gsp.remove_stopwords,           \n",
        "               gsp.strip_short,               \n",
        "               gsp.stem_text                    \n",
        "      ]"
      ],
      "metadata": {
        "id": "lIQrzPmOCtDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data cleaning"
      ],
      "metadata": {
        "id": "RHINjcQ8C2-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame(columns=(['text']))\n",
        "for i in range(0,len(data1)):\n",
        "    a=data1.iloc[i]['text']\n",
        "    a=a.lower()\n",
        "    a=utils.to_unicode(a)\n",
        "    for f in filters:\n",
        "        a=f(a)\n",
        "    d = pd.DataFrame({'text': a}, index=[0])\n",
        "    data = data.append(d, ignore_index=True)\n",
        "\n",
        "data['mark']=label"
      ],
      "metadata": {
        "id": "8h4-PHzdC2XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_set_spilt"
      ],
      "metadata": {
        "id": "FrOBfjuRFttN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(data['text'],data['mark'],test_size=0.2 , stratify=data['mark'],random_state=389 )"
      ],
      "metadata": {
        "id": "KR0lTTZeFuEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "word frequency"
      ],
      "metadata": {
        "id": "RAqOoEJVGzIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get feature\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def get_list_tokens(string):\n",
        "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
        "  list_tokens=[]\n",
        "  for sentence in sentence_split:\n",
        "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "    for token in list_tokens_sentence:\n",
        "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "  return list_tokens\n",
        "\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "\n",
        "dict_word_frequency={}\n",
        "\n",
        "for i in x_train:\n",
        "    sentence_tokens = get_list_tokens(i)\n",
        "    for word in sentence_tokens:\n",
        "        if word in stopwords: continue\n",
        "        if word not in dict_word_frequency:\n",
        "            dict_word_frequency[word] = 1\n",
        "        else:\n",
        "            dict_word_frequency[word] += 1\n",
        "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000]\n",
        "\n",
        "vocabulary=[]\n",
        "for word,frequency in sorted_list:\n",
        "  vocabulary.append(word)\n",
        "\n",
        "\n",
        "# transform sentences into vectors\n",
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text\n",
        "\n",
        "df_train=[]\n",
        "df_test=[]\n",
        "x_test_1=[]\n",
        "for i in x_train:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    df_train.append(vector_review)\n",
        "for i in x_test:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    df_test.append(vector_review)\n",
        "for i in x_test:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    x_test_1.append(vector_review)\n",
        "X_train_sentanalysis=np.asarray(df_train)\n",
        "Y_train_sentanalysis=np.asarray(y_train)\n",
        "x_test_1_sentanalysis=np.asarray(x_test_1)\n",
        "\n",
        "fs_sentanalysis=SelectKBest(chi2,k=500).fit(X_train_sentanalysis,Y_train_sentanalysis)\n",
        "X_train_sentanalysis_new=fs_sentanalysis.transform(X_train_sentanalysis)\n",
        "x_test_sentanalysis_new=fs_sentanalysis.transform(x_test_1_sentanalysis)\n",
        "\n",
        "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "svm_clf_sentanalysis.fit(X_train_sentanalysis_new,Y_train_sentanalysis)\n",
        "print('word frequency')\n",
        "print(classification_report(y_test,svm_clf_sentanalysis.predict(x_test_sentanalysis_new)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx1SIx4MG2Av",
        "outputId": "2be8a8c7-af4c-459b-f4a9-56ac4dc9e782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word frequency\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.95       102\n",
            "           1       0.90      0.97      0.94        77\n",
            "           2       0.96      0.90      0.93        84\n",
            "           3       0.96      0.99      0.98       102\n",
            "           4       0.99      0.95      0.97        80\n",
            "\n",
            "    accuracy                           0.95       445\n",
            "   macro avg       0.95      0.95      0.95       445\n",
            "weighted avg       0.95      0.95      0.95       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF"
      ],
      "metadata": {
        "id": "J5zIalqLJKN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=TfidfVectorizer(ngram_range=(1,2),max_features=20,stop_words=['english'],max_df=0.6)\n",
        "x_train_new=tfidf.fit_transform(x_train)\n",
        "x_test_new=tfidf.fit_transform(x_test)\n",
        "#feature selection\n",
        "fs_sentanalysis=SelectKBest(chi2,k=10).fit(x_train_new,y_train)\n",
        "x_train_1=fs_sentanalysis.transform(x_train_new)\n",
        "x_test_1=fs_sentanalysis.transform(x_test_new)\n",
        "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "svm_clf_sentanalysis.fit(x_train_new,y_train)\n",
        "print(classification_report(y_test,svm_clf_sentanalysis.predict(x_test_new)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LruPjJq4G19A",
        "outputId": "2c88ebdf-3fba-405e-fb6b-851e60c42716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.53      0.59      0.56       102\n",
            "           1       0.48      0.53      0.51        77\n",
            "           2       0.57      0.38      0.46        84\n",
            "           3       0.68      0.72      0.70       102\n",
            "           4       0.44      0.46      0.45        80\n",
            "\n",
            "    accuracy                           0.55       445\n",
            "   macro avg       0.54      0.54      0.53       445\n",
            "weighted avg       0.55      0.55      0.54       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bio_gram"
      ],
      "metadata": {
        "id": "f3YOo_Ycn3qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel = SelectKBest(chi2, k=3000)\n",
        "pipe=Pipeline([('vec',CountVectorizer(ngram_range=(2,2),min_df=3,max_df=0.9,max_features=4500)),\n",
        "               ('sel',sel),\n",
        "               ('clf',LogisticRegression(C=4,dual=False,max_iter=10000))])\n",
        "clf=pipe.fit(x_train,y_train)\n",
        "print('bio_gram')\n",
        "print(classification_report(y_test,clf.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOZcV3_inzFI",
        "outputId": "530a4c5b-117b-4e9d-b85b-5e160744353a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bio_gram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.91       102\n",
            "           1       0.85      0.94      0.89        77\n",
            "           2       0.91      0.87      0.89        84\n",
            "           3       0.91      0.97      0.94       102\n",
            "           4       0.94      0.82      0.88        80\n",
            "\n",
            "    accuracy                           0.90       445\n",
            "   macro avg       0.90      0.90      0.90       445\n",
            "weighted avg       0.91      0.90      0.90       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tri_gram"
      ],
      "metadata": {
        "id": "VmrR4G8EnyzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel=SelectKBest(chi2,k=2500)\n",
        "pipe=Pipeline([('vec',CountVectorizer(ngram_range=(3,3),min_df=3,max_df=0.9,max_features=3000)),\n",
        "        ('sel',sel),\n",
        "        ('clf',LogisticRegression(C=4,dual=False,max_iter=10000))])\n",
        "clf=pipe.fit(x_train,y_train)\n",
        "print('tri_gram')\n",
        "print(classification_report(y_test,clf.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXI-iO9xnyeP",
        "outputId": "ec3cb5a0-0a25-4e62-bf11-154d3ff5541c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tri_gram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.57      0.71       102\n",
            "           1       0.38      0.97      0.55        77\n",
            "           2       0.97      0.69      0.81        84\n",
            "           3       0.99      0.68      0.80       102\n",
            "           4       0.91      0.66      0.77        80\n",
            "\n",
            "    accuracy                           0.70       445\n",
            "   macro avg       0.84      0.71      0.73       445\n",
            "weighted avg       0.86      0.70      0.73       445\n",
            "\n"
          ]
        }
      ]
    }
  ]
}