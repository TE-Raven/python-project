{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "import moduls"
      ],
      "metadata": {
        "id": "vHjyjYhRqpHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "WXdbsy_3p2O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6831f7-c2ec-42a2-c542-2e742bf76765"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive #connect to colab\n",
        "import numpy as np #data processing tools\n",
        "import pandas as pd #data processing tools\n",
        "import os #for file reading\n",
        "from sklearn.model_selection import train_test_split #Divide the dataset\n",
        "from gensim import utils #unicode encoding conversion\n",
        "import gensim.parsing.preprocessing as gsp #Data cleaning\n",
        "import nltk  #Data cleaning\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import sklearn #machine learning library\n",
        "import operator # The itemgetter function provided by the operator module is used to obtain the data of which dimensions of the object, and the parameters are some serial numbers\n",
        "drive.mount('/content/drive') #cloud disk path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer #tfidf feature extraction library\n",
        "from sklearn import svm #svm classifier\n",
        "from sklearn.linear_model import LogisticRegression #logistic regression classifier\n",
        "from sklearn.metrics import classification_report #Classification performance report\n",
        "from sklearn.feature_selection import chi2 #Feature selection filter -chi2 (chi-square statistic)\n",
        "from sklearn.feature_selection import SelectKBest #Score features and select features from high to low\n",
        "from sklearn.feature_extraction.text import CountVectorizer #Numerical computation of features\n",
        "from sklearn.pipeline import Pipeline #The training data and test data are processed in a unified way"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataprocessing"
      ],
      "metadata": {
        "id": "qJxnVtbRqmkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root=r\"/content/drive/MyDrive/news_classifacation/bbc\"\n",
        "filename=[]\n",
        "dirs=os.listdir(root)#Get the folder name in the path\n",
        "mark=0 #Label\n",
        "data= pd.DataFrame(columns=('text','mark'))#Create a dataframe and use the text column to store the data, and the mark column to store the label\n",
        "#print(dirs)\n",
        "for dir in dirs:\n",
        "    dir_path = root +'/' +dir #Get dataset folder path\n",
        "    names = os.listdir(dir_path) #Get a list of dataset txt file names\n",
        "    if dir == 'business':\n",
        "        mark =0\n",
        "    if dir == 'entertainment':\n",
        "        mark =1\n",
        "    if dir == 'politics':\n",
        "        mark =2\n",
        "    if dir == 'sport':\n",
        "        mark =3\n",
        "    if dir == 'tech':\n",
        "        mark =4\n",
        "    for n in names:\n",
        "     filename.append(dir_path + '/'+ n )# Get the data set txt file path\n",
        "    for m in filename:\n",
        "        with open(m, encoding='gb18030', errors='ignore') as content:\n",
        "            # text=content.rstrip('\\n')\n",
        "            a = content.read() #Read dataset file contents\n",
        "            b = a.replace('\\n', '').replace('\\r', '') #remove newlines\n",
        "            data1 = pd.DataFrame({'text': b, \"mark\": mark}, index=[0])#write to temporary dataframe\n",
        "            data = data.append(data1, ignore_index=True) #Add to the dataset dataframe\n",
        "    filename=[]\n",
        "#data.to_csv(\"dataset.csv\") this command can generate a dataset file for check or other using\n"
      ],
      "metadata": {
        "id": "dV2m86M5p5vt"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1=data[['text','mark']] #Get the dataset dataframe\n",
        "lable=data['mark'] #get label"
      ],
      "metadata": {
        "id": "u3WuIwgus35h"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set filters"
      ],
      "metadata": {
        "id": "zc7dsoJtCtuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filters = [\n",
        "               gsp.strip_punctuation,     # remove punctuation     \n",
        "               gsp.strip_multiple_whitespaces, # delete multiple lines \n",
        "               gsp.strip_numeric,       # delete numbers      \n",
        "               gsp.remove_stopwords,      # Remove stop words (and, to, the in English)    \n",
        "               gsp.strip_short,        # lowercase formal words      \n",
        "               gsp.stem_text          # Extract stem to etymological form          \n",
        "      ]"
      ],
      "metadata": {
        "id": "lIQrzPmOCtDj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data cleaning"
      ],
      "metadata": {
        "id": "RHINjcQ8C2-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.DataFrame(columns=(['text']))\n",
        "for i in range(0,len(data1)):\n",
        "    a=data1.iloc[i]['text']\n",
        "    a=a.lower()\n",
        "    a=utils.to_unicode(a)#unicode encoding conversion\n",
        "    for f in filters: #text cleaning\n",
        "        a=f(a)\n",
        "    d = pd.DataFrame({'text': a}, index=[0])\n",
        "    data = data.append(d, ignore_index=True) #Add the cleaned text back to the dataframe\n",
        "\n",
        "data['mark']=lable#rewrite the labels to the dataframe"
      ],
      "metadata": {
        "id": "8h4-PHzdC2XL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data_set_spilt"
      ],
      "metadata": {
        "id": "FrOBfjuRFttN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(data['text'],data['mark'],test_size=0.2 , stratify=data['mark'],random_state=389 )"
      ],
      "metadata": {
        "id": "KR0lTTZeFuEv"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_train,X_test, y_train, y_test =sklearn.model_selection.train_test_split(train_data,train_target,test_size=0.4, random_state=0,stratify=y_train)\n",
        "train_data: The sample feature set to be divided\n",
        "train_target: The sample result to be divided\n",
        "test_size: sample proportion, if it is an integer, it is the number of samples\n",
        "random_state: is the seed of the random number.\n",
        "\n",
        "Random number seed: In fact, it is the number of the random number of the group. When the experiment needs to be repeated, it is guaranteed to get the same set of random numbers. For example, if you fill in 1 every time, the random array you get is the same when the other parameters are the same. But fill in 0 or not fill in, each time will be different.\n",
        "\n",
        "stratify is to preserve the distribution of the pre-split classes. For example, there are 100 pieces of data, 80 belong to class A and 20 belong to class B. If train_test_split(... test_size=0.25, stratify = y_all), then the data after split is as follows:\n",
        "training: 75 data, of which 60 belong to class A and 15 belong to class B.\n",
        "testing: 25 data, of which 20 belong to class A and 5 belong to class B.\n",
        "With the stratify parameter, the ratio of the classes in the training set and the testing set is A:B=4:1, which is equivalent to the ratio before split (80:20). Stratify is usually used in such cases where the class distribution is unbalanced.\n",
        "\n",
        "Stratify=X is to distribute according to the proportion in X\n",
        "Stratify=y is to distribute according to the proportion in y"
      ],
      "metadata": {
        "id": "FRFQHf8k7KR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word frequency"
      ],
      "metadata": {
        "id": "RAqOoEJVGzIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get feature\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()#Use the lemmatization tool to combine different spellings of the same root.\n",
        "\n",
        "def get_list_tokens(string): #get list tokens\n",
        "  sentence_split=nltk.tokenize.sent_tokenize(string)\n",
        "  list_tokens=[]\n",
        "  for sentence in sentence_split:\n",
        "    list_tokens_sentence=nltk.tokenize.word_tokenize(sentence)\n",
        "    for token in list_tokens_sentence:\n",
        "      list_tokens.append(lemmatizer.lemmatize(token).lower())\n",
        "  return list_tokens\n",
        "\n",
        "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
        "stopwords.add(\".\")\n",
        "stopwords.add(\",\")\n",
        "stopwords.add(\"--\")\n",
        "stopwords.add(\"``\")\n",
        "\n",
        "dict_word_frequency={}\n",
        "#get word frequency in text\n",
        "for i in x_train:\n",
        "    sentence_tokens = get_list_tokens(i)\n",
        "    for word in sentence_tokens:\n",
        "        if word in stopwords: continue\n",
        "        if word not in dict_word_frequency:\n",
        "            dict_word_frequency[word] = 1\n",
        "        else:\n",
        "            dict_word_frequency[word] += 1\n",
        "sorted_list = sorted(dict_word_frequency.items(), key=operator.itemgetter(1), reverse=True)[:1000] #Get the top 1000 words of frequency\n",
        "\n",
        "vocabulary=[] #write word to table\n",
        "for word,frequency in sorted_list:\n",
        "  vocabulary.append(word)\n",
        "\n",
        "\n",
        "# transform sentences into vectors\n",
        "def get_vector_text(list_vocab,string):\n",
        "  vector_text=np.zeros(len(list_vocab))\n",
        "  list_tokens_string=get_list_tokens(string)\n",
        "  for i, word in enumerate(list_vocab):\n",
        "    if word in list_tokens_string:\n",
        "      vector_text[i]=list_tokens_string.count(word)\n",
        "  return vector_text\n",
        "\n",
        "df_train=[]\n",
        "df_test=[]\n",
        "x_test_1=[]\n",
        "for i in x_train:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    df_train.append(vector_review)\n",
        "for i in x_test:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    df_test.append(vector_review)\n",
        "for i in x_test:\n",
        "    vector_review = get_vector_text(vocabulary, i)\n",
        "    x_test_1.append(vector_review)\n",
        "\n",
        "#convert to array\n",
        "X_train_sentanalysis=np.asarray(df_train)\n",
        "Y_train_sentanalysis=np.asarray(y_train)\n",
        "x_test_1_sentanalysis=np.asarray(x_test_1)\n",
        "\n",
        "#Feature selection, use chi-square for feature selection, select the top 500 features as training features\n",
        "#The value of k is to select the largest number of features\n",
        "fs_sentanalysis=SelectKBest(chi2,k=500).fit(X_train_sentanalysis,Y_train_sentanalysis.astype('int'))\n",
        "X_train_sentanalysis_new=fs_sentanalysis.transform(X_train_sentanalysis)\n",
        "x_test_sentanalysis_new=fs_sentanalysis.transform(x_test_1_sentanalysis)\n",
        "\n",
        "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "#The type of sum function used in the algorithm, the kernel function is a method used to transform a nonlinear problem into a linear problem. The parameter options are RBF, Linear, Poly, Sigmoid, precomputed or a custom kernel function. The default is \"RBF\", which is the radial basis kernel, which is the Gaussian kernel function; Linear refers to the linear kernel function, and Poly refers to is a polynomial kernel, and Sigmoid refers to the hyperbolic tangent function tanh kernel;\n",
        "#Kernel function coefficient, this parameter is the kernel coefficient of rbf, poly and sigmoid; the default is 'auto', then the inverse of the number of features will be used, ie 1 / n_features. (i.e. the bandwidth of the kernel function, the radius of the hypercircle). The larger the gamma, the smaller the σ, which makes the Gaussian distribution tall and thin, so that the model can only act near the support vector, which may lead to overfitting; on the contrary, the smaller the gamma, the larger the σ, and the Gaussian distribution will be too smooth. Poor classification on the set may lead to underfitting.\n",
        "svm_clf_sentanalysis.fit(X_train_sentanalysis_new,Y_train_sentanalysis.astype('int')) #Train the model\n",
        "print('word frequency')\n",
        "print(classification_report(y_test.astype('int'),svm_clf_sentanalysis.predict(x_test_sentanalysis_new)))#The result is obtained by the classification_report function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx1SIx4MG2Av",
        "outputId": "839088f5-719c-4914-9108-fa088322caed"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word frequency\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.95       102\n",
            "           1       0.92      1.00      0.96        77\n",
            "           2       0.96      0.92      0.94        84\n",
            "           3       0.97      0.99      0.98       102\n",
            "           4       0.97      0.95      0.96        80\n",
            "\n",
            "    accuracy                           0.96       445\n",
            "   macro avg       0.96      0.96      0.96       445\n",
            "weighted avg       0.96      0.96      0.96       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TFIDF"
      ],
      "metadata": {
        "id": "J5zIalqLJKN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf=TfidfVectorizer(ngram_range=(1,2),max_features=20,stop_words=['english'],max_df=0.6)\n",
        "x_train_new=tfidf.fit_transform(x_train)#Get the word frequency inverse document frequency feature\n",
        "x_test_new=tfidf.fit_transform(x_test)#Get the word frequency inverse document frequency feature\n",
        "#feature selection\n",
        "fs_sentanalysis=SelectKBest(chi2,k=10).fit(x_train_new,y_train.astype('int'))\n",
        "x_train_1=fs_sentanalysis.transform(x_train_new)\n",
        "x_test_1=fs_sentanalysis.transform(x_test_new)\n",
        "svm_clf_sentanalysis=sklearn.svm.SVC(kernel=\"linear\",gamma='auto')\n",
        "\n",
        "\n",
        "svm_clf_sentanalysis.fit(x_train_new,y_train.astype('int'))#train the Model\n",
        "\n",
        "print(classification_report(y_test.astype('int'),svm_clf_sentanalysis.predict(x_test_new)))#The result is obtained by the classification_report function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LruPjJq4G19A",
        "outputId": "fac40762-8f3a-4e07-bad6-8679d96c70ba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.52      0.54       102\n",
            "           1       0.56      0.70      0.62        77\n",
            "           2       0.62      0.54      0.57        84\n",
            "           3       0.63      0.61      0.62       102\n",
            "           4       0.41      0.42      0.42        80\n",
            "\n",
            "    accuracy                           0.56       445\n",
            "   macro avg       0.56      0.56      0.55       445\n",
            "weighted avg       0.56      0.56      0.56       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer Parameter introduction"
      ],
      "metadata": {
        "id": "vEdephZyD7cJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input: string{'filename', 'file', 'content'}\n",
        "\n",
        "    If 'filename', the sequence is passed as an argument to the fitter, expected to be a list of filenames, which needs to read the raw content for analysis\n",
        "\n",
        "    If 'file', the sequence item must have a 'read' method (a file-like object) that is called to get the number of bytes in memory\n",
        "\n",
        "    Otherwise, the input is expected to be a sequence string, or byte data items are expected to be parsed directly.\n",
        "\n",
        "encoding: string, 'utf-8' by default\n",
        "\n",
        "    If given bytes or files to parse, this encoding will be used for decoding\n",
        "\n",
        "decode_error: {'strict', 'ignore', 'replace'}\n",
        "\n",
        "    Indicates what to do if a given byte sequence contains characters that are not in the given encoding. By default it is 'strict' which means UnicodeDecodeError will be raised, other values ​​are 'ignore' and 'replace'\n",
        "\n",
        "strip_accents: {'ascii', 'unicode', None}\n",
        "\n",
        "    Remove encoding rules (accents) in the preprocessing step, \"ASCII\" is a fast method, only if there is a direct ASCII character mapping, \"unicode\" is a slightly slower method, None (default) what neither do\n",
        "\n",
        "analyzer: string, {'word', 'char'} or callable\n",
        "\n",
        "    Defines features as words or n-gram characters, if the call passed to it is used to extract feature sequences from unprocessed input source files\n",
        "\n",
        "preprocessor: callable or None (default)\n",
        "\n",
        "    Override the preprocessing (string transformation) stage when preserving tokens and \"n-gram\" generation steps\n",
        "\n",
        "tokenizer: callable or None (default)\n",
        "\n",
        "    Override the string token step when keeping the preprocessing and n-gram generation steps\n",
        "\n",
        "ngram_range: tuple(min_n, max_n)\n",
        "\n",
        "    Lower and upper range of n-values ​​of n-grams to be extracted, all values ​​of n in the interval min_n <= n <= max_n\n",
        "\n",
        "stop_words: string {'english'}, list, or None (default)\n",
        "\n",
        "    If not english, the built-in stopword list for English\n",
        "\n",
        "    If not listed, the list is assumed to contain stop words and all words in the list will be removed from the token\n",
        "\n",
        "    If None, stopwords are not used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on internal expected word frequencies\n",
        "\n",
        "lowercase: boolean, default True\n",
        "\n",
        "    Convert all characters to lowercase before tokenization\n",
        "\n",
        "token_pattern: string\n",
        "\n",
        "    The regular expression showing the composition of \"token\" is only used when analyzer == 'word'. Regular expression of two or more alphanumeric characters (punctuation is completely ignored, always treated as a token separator).\n",
        "\n",
        "max_df: float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
        "\n",
        "    When building the vocabulary, terms with document frequencies above a given threshold are strictly ignored, corpus-specified stopwords. If it is a floating point value, this parameter represents the proportion of the document, an integer absolute count value, if the vocabulary is not None, this parameter is ignored.\n",
        "\n",
        "min_df: float in range [0.0, 1.0] or int, optional, 1.0 by default\n",
        "\n",
        "When building the vocabulary, terms with a document frequency lower than a given threshold are strictly ignored, stopwords specified by the corpus. If it is a floating point value, this parameter represents the proportion of the document, an integer absolute count value, if the vocabulary is not None, this parameter is ignored.\n",
        "\n",
        "max_features: optional, None by default\n",
        "\n",
        "    If not None, build a vocabulary, only consider max_features--sort by corpus word frequency, if the vocabulary is not None, this parameter is ignored\n",
        "\n",
        "vocabulary: Mapping or iterable, optional\n",
        "\n",
        "    Also a Map (eg, a dictionary) where the keys are the terms and the values ​​are indices in the feature matrix, or iterators in the terms. If not given, the vocabulary is determined from the input file. There must be no duplication of indices in the map, and no gaps between 0 and the maximum index value.\n",
        "\n",
        "binary: boolean, False by default\n",
        "\n",
        "    If not True, all non-zero counts are set to 1, which is useful for discrete probability models, modeling binary events instead of integer counts\n",
        "\n",
        "dtype: type, optional\n",
        "\n",
        "    The type of the matrix returned by fit_transform() or transform()\n",
        "\n",
        "norm: 'l1', 'l2', or None, optional\n",
        "\n",
        "    Norms are used to normalize term vectors. None for non-normalization\n",
        "\n",
        "use_idf: boolean, optional\n",
        "\n",
        "    Start inverse-document-frequency to recalculate weights\n",
        "\n",
        "smooth_idf: boolean, optional\n",
        "\n",
        "    Smooth the idf weights by adding 1 to the document frequency, adding an extra document to prevent division by zero\n",
        "\n",
        "sublinear_tf: boolean, optional\n",
        "\n",
        "    Apply a linear scaling TF, e.g. overwrite tf with 1+log(tf)"
      ],
      "metadata": {
        "id": "JG_Nhz3uDvzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bio_gram"
      ],
      "metadata": {
        "id": "f3YOo_Ycn3qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel = SelectKBest(chi2, k=3000)#Define Feature Selection Parameters\n",
        "pipe=Pipeline([('vec',CountVectorizer(ngram_range=(2,2),min_df=3,max_df=0.9,max_features=4500)),\n",
        "               ('sel',sel),\n",
        "               ('clf',LogisticRegression(C=4,dual=False,max_iter=10000))])\n",
        "clf=pipe.fit(x_train,y_train.astype('int'))\n",
        "print('bio_gram')\n",
        "print(classification_report(y_test.astype('int'),clf.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOZcV3_inzFI",
        "outputId": "065c9625-4b48-4721-f717-3a56612ed3e4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bio_gram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92       102\n",
            "           1       0.87      0.94      0.90        77\n",
            "           2       0.96      0.88      0.92        84\n",
            "           3       0.88      0.97      0.92       102\n",
            "           4       0.97      0.85      0.91        80\n",
            "\n",
            "    accuracy                           0.91       445\n",
            "   macro avg       0.92      0.91      0.91       445\n",
            "weighted avg       0.92      0.91      0.91       445\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer parameter introduction"
      ],
      "metadata": {
        "id": "m9PQukKEEycg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "input: string {'filename', 'file', 'content'}\n",
        "\n",
        "If 'filename', passed as argument in a suitable sequence is expected to be a list of filenames that need to be read to get the raw content to parse.\n",
        "If 'file', the sequence item must have a 'read' method (file-like object) that is called to get the bytes in memory.\n",
        "Otherwise, the expected input is a sequence string or bytes items are expected to be parsed directly.\n",
        "encoding: string, defaults to 'utf-8'.\n",
        "\n",
        "If bytes or files are given for analysis, this encoding is used for decoding.\n",
        "decode_error: {'strict', 'ignore', 'replace'}\n",
        "\n",
        "Indicates what to do if a byte sequence is given to parse containing characters that are not in the given encoding. By default, it is 'strict', which means UnicodeDecodeError is raised. Other values ​​are \"ignore\" and \"replace\".\n",
        "strip_accents: {'ascii', 'unicode', None}\n",
        "\n",
        "Whether to remove accents in the preprocessing step. 'ascii' is a fast method that only works for characters with direct ASCII mapping. 'unicode' is a slightly slower method that works for any character. None (default) does nothing.\n",
        "analyzer: string, {'word', 'char', 'char_wb'} or callable\n",
        "\n",
        "Whether the feature should consist of word or character n-grams. Option 'char_wb' creates character n-grams only from text within word boundaries; n-grams at word edges are padded with spaces.\n",
        "If passed a callable, it will be used to extract the sequence of features from the raw unprocessed input.\n",
        "preprocessor: callable or None (default)\n",
        "\n",
        "Overrides the preprocessing (string transformation) stage while retaining the tokenizing and n-grams generation steps.\n",
        "tokenizer: callable or None (default)\n",
        "\n",
        "Rewrite the string tokenization step while preserving the preprocessing and n-grams generation steps.\n",
        "only works with analyzer == 'word'\n",
        "ngram_range: tuple(min_n, max_n)\n",
        "\n",
        "Lower and upper bounds of the range of n values ​​for the different n-grams to extract.\n",
        "All values ​​of n will be used such that min_n <= n <= max_n.\n",
        "stop_words: string {'english'}, list, or None (default)\n",
        "\n",
        "\"english\", using the built-in English stopword list.\n",
        "\"list\", if a list, is assumed to contain stop words, then all words in the list will be removed from the resulting token. only works with analyzer == 'word'\n",
        "\"None\", stopwords will not be processed. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on term internal corpus document frequency.\n",
        "lowercase: boolean, True (default)\n",
        "\n",
        "Convert all characters to lowercase before calculating 'tf'.\n",
        "token_pattern: string\n",
        "\n",
        "Regular expression that defaults to filtering mixed alphanumeric characters of length >= 2 (punctuation is completely ignored and always treated as a token separator). Only used when analyzer=='word' is used.\n",
        "max_df: float (in [0.0, 1.0]), or int, default = 1.0\n",
        "\n",
        "Words with a document frequency strictly above a given threshold (corpus-specific stopwords) are ignored when building the vocabulary. If float, the parameter represents the scale of the document, i.e. an integer absolute count.\n",
        "This parameter is ignored if vocabulary is not None.\n",
        "min_df: float (in [0.0, 1.0]) or int, default = 1\n",
        "\n",
        "If an int, words with a document frequency strictly below the given threshold are ignored when building the vocabulary. This value is also called cutoff value in the literature;\n",
        "If float, the parameter represents the scale of the document;\n",
        "This parameter is ignored if vocabulary is not None.\n",
        "max_features: int or None (default)\n",
        "\n",
        "If not None, build a vocabulary and create a corpus with only the top max_features words sorted by word frequency.\n",
        "This parameter is ignored if vocabulary is not None.\n",
        "vocabulary: Mapping or iterable\n",
        "\n",
        "Mapping (for example, a dictionary) where keys are terms and values ​​are indices in the feature matrix;\n",
        "iterable, an iterable over terms;\n",
        "If not given, the vocabulary is determined from the input file. mapping, the exponents should not be repeated and there should not be any gap between 0 and the maximum exponent.\n",
        "binary: boolean, default is False\n",
        "\n",
        "If True, all non-zero counts are set to 1. (i.e., tf has only values ​​0 and 1, indicating occurrences and non-occurrences)\n",
        "This is useful for discrete probability models that model binary events rather than integer counts.\n",
        "dtype: type, optional\n",
        "\n",
        "The type of matrix returned by fit_transform() or transform()."
      ],
      "metadata": {
        "id": "M2jLpqcyEy3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introduction to Pipeline Mechanism"
      ],
      "metadata": {
        "id": "A7MNJz8SFbE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline mechanism realizes the streaming encapsulation and management of all steps\n",
        "Steps that can be placed in the Pipeline may be:\n",
        "Feature standardization can be used as the first link\n",
        "Data dimensionality reduction (feature selection) can be added in the middle\n",
        "Defining the classifier is the last step"
      ],
      "metadata": {
        "id": "zwQs_CKQEyOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "tri_gram"
      ],
      "metadata": {
        "id": "VmrR4G8EnyzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sel=SelectKBest(chi2,k=2500)\n",
        "pipe=Pipeline([('vec',CountVectorizer(ngram_range=(3,3),min_df=3,max_df=0.9,max_features=3000)),\n",
        "        ('sel',sel),\n",
        "        ('clf',LogisticRegression(C=4,dual=False,max_iter=10000))])\n",
        "clf=pipe.fit(x_train,y_train.astype('int'))\n",
        "print('tri_gram')\n",
        "print(classification_report(y_test.astype('int'),clf.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXI-iO9xnyeP",
        "outputId": "efa372d2-1c65-4399-bd4e-9fa8e3cc7d03"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tri_gram\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.58      0.72       102\n",
            "           1       0.39      0.96      0.56        77\n",
            "           2       0.97      0.71      0.82        84\n",
            "           3       0.96      0.70      0.81       102\n",
            "           4       0.93      0.66      0.77        80\n",
            "\n",
            "    accuracy                           0.71       445\n",
            "   macro avg       0.84      0.72      0.73       445\n",
            "weighted avg       0.85      0.71      0.74       445\n",
            "\n"
          ]
        }
      ]
    }
  ]
}